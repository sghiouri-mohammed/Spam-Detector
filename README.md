# ğŸ“§ Spam Detector

DÃ©tection de spams Ã  partir de texte dâ€™email via un pipeline ML scikitâ€‘learn (TFâ€‘IDF + modÃ¨les linÃ©aires/SVM/NB) avec optimisation par validation croisÃ©e, sÃ©lection de seuil optimale et application Streamlit pour lâ€™infÃ©rence.

Le projet fournit:
- un notebook dâ€™exploration et dâ€™expÃ©rimentation `spam_emails_detection.ipynb`
- un package dâ€™entraÃ®nement/Ã©valuation scriptable dans `src/`
- une app `Streamlit` dans `app/` qui charge le modÃ¨le et applique un seuil appris
- des donnÃ©es dâ€™exemple dans `data/` (suivies par DVC pour la source `emails.csv`)

---

## ğŸ§­ Structure du projet

```
Quality and Strategy/
  app/
    streamlit_app.py           # UI pour lâ€™infÃ©rence: texte unique et CSV + visualisations
  data/
    emails.csv                 # Dataset text,label (ham/spam) â€” versionnÃ© via DVC
  models/
    model.pkl                  # Pipeline sklearn (TFâ€‘IDF + meilleur modÃ¨le)
    threshold.txt              # Seuil de dÃ©cision optimisÃ© (validation)
  src/
    prepare_data.py            # Chargement + splits stratifiÃ©s (train/val/test)
    train.py                   # GridSearchCV multiâ€‘modÃ¨les + sÃ©lection de seuil + sauvegarde
    evaluate.py                # Rapport sur un CSV avec seuil (mÃ©triques + CM)
  spam_emails_detection.ipynb  # Notebook complet (EDA, entrainement, comparaisons, courbes)
  tests/
    test_model.py              # Tests basiques (ex: chargement modÃ¨le)
  requirements.txt             # DÃ©pendances
  dvc.yaml / data/*.dvc        # TraÃ§abilitÃ© des donnÃ©es (optionnel)
```

---

## ğŸ§ª DonnÃ©es (`data/`)

- Fichier principal: `data/emails.csv` avec les colonnes `text` et `label` (valeurs `ham` ou `spam`).
- Dans le code, on binarise la cible: `label_bin = (label.lower() == "spam").astype(int)`, donc 1=spam, 0=ham.
- Les splits sont stratifiÃ©s pour conserver la proportion de spams dans train/val/test.

---

## ğŸ”§ PrÃ©paration et splits (`src/prepare_data.py`)

RÃ´les clÃ©s:
- Chargement robuste du CSV (`load_dataset`) et nettoyage minimal (drop `NaN`, cast `text` en `str`).
- CrÃ©ation de la cible binaire `label_bin`.
- Splits stratifiÃ©s et reproductibles avec `RANDOM_STATE=42`:
  - 80% train/val, 20% test
  - puis 75% train / 25% val sur le bloc train/val (â‰ˆ 60/20/20 global)

API principale:
- `load_dataset(path: Path) -> pd.DataFrame`
- `stratified_splits(df) -> (X_train, X_val, X_test, y_train, y_val, y_test)`

---

## ğŸ§  EntraÃ®nement et optimisation (`src/train.py`)

Objectifs:
- Construire un pipeline `TfidfVectorizer` + estimateur.
- Ã‰valuer plusieurs familles de modÃ¨les via `GridSearchCV` (score AP / Average Precision):
  - `LogisticRegression` (class_weight="balanced")
  - `LinearSVC` calibrÃ© via `CalibratedClassifierCV`
  - `MultinomialNB`
  - `SVC` RBF (class_weight="balanced")
- SÃ©lectionner le meilleur pipeline sur la moyenne de lâ€™AP en CV.
- Choisir un seuil de dÃ©cision optimale sur la validation (max F1 sur courbe Precisionâ€‘Recall).
- Ã‰valuer sur test avec ce seuil, puis sauver `models/model.pkl` et `models/threshold.txt`.

Points importants:
- Gestion du dÃ©sÃ©quilibre: `class_weight="balanced"` pour LR/SVM; calibration probabiliste pour LinearSVC.
- Grilles hyperparamÃ¨tres compactes mais efficaces (min_df, ngram_range, C/alpha/solverâ€¦).
- Normalisation de score fallback: si `predict_proba` indisponible, on utilise `decision_function` + minâ€‘max.

ExÃ©cution:
```bash
python -m src.train
```
Sorties:
- `models/model.pkl`: pipeline sklearn entraÃ®nÃ©
- `models/threshold.txt`: seuil (float) choisi via la validation
- Impression des mÃ©triques test (accuracy, precision, recall, F1, ROC AUC, AP)

---

## ğŸ“Š Ã‰valuation (`src/evaluate.py`)

Fonctions:
- Charge `models/model.pkl` et le `threshold.txt`.
- Lit un CSV (par dÃ©faut `data/emails.csv`).
- Calcule un score par email, applique le seuil, affiche:
  - classification report (precision/recall/F1/support)
  - matrice de confusion + faux positifs
  - mÃ©triques agrÃ©gÃ©es (accuracy/precision/recall/F1/ROC AUC/AP + seuil)

ExÃ©cution:
```bash
python -m src.evaluate
```

---

## ğŸ““ Notebook dâ€™expÃ©rimentation (`spam_emails_detection.ipynb`)

Contenu principal:
- EDA rapide: rÃ©partition ham/spam, pie chart, parts de classes.
- Baseline: TFâ€‘IDF + Logistic Regression (rÃ©fÃ©rence).
- Comparaison multiâ€‘modÃ¨les (LR, LinearSVC, SVM RBF, MultinomialNB, RandomForest) + courbes ROC/PR.
- Pipeline GridSearch multiâ€‘modÃ¨les (scoring AP) + sÃ©lection de seuil optimal sur validation.
- Ã‰valuation test dÃ©taillÃ©e: report, matrice de confusion, ROC/PR, seuil utilisÃ©.
- Analyse dâ€™erreurs (faux positifs / faux nÃ©gatifs marquants) pour guider lâ€™itÃ©ration.
- Sauvegarde du meilleur pipeline et du seuil (synchronisÃ© avec `src/train.py`).

Le notebook sert de rÃ©fÃ©rence lisible et dâ€™espace dâ€™itÃ©ration, alors que `src/` est prÃªt pour lâ€™industrialisation.

---

## ğŸ–¥ï¸ Application Streamlit (`app/streamlit_app.py`)

FonctionnalitÃ©s:
- Chargement du modÃ¨le `models/model.pkl` (mise en cache `@st.cache_resource`).
- Chargement du seuil `models/threshold.txt` (fallback 0.5 si absent).
- Inference sur un texte saisi: score spam via `predict_proba` si dispo, sinon `decision_function` minâ€‘max.
- Application du seuil appris pour dÃ©cider SPAM/HAM (Ã©vite le biais dâ€™un 0.5 fixe).
- Upload CSV `text[,label]` et prÃ©dictions batch:
  - colonnes ajoutÃ©es: `score_spam` (si dispo), `prediction` (spam/ham)
  - visualisations: 
    - rÃ©partition des labels vrais (camembert) si `label` prÃ©sent
    - rÃ©partition des prÃ©dictions (bar chart)
    - distribution des scores + ligne de seuil
    - mÃ©triques agrÃ©gÃ©es (Accuracy, Precision, Recall, F1) si `label` prÃ©sent
    - matrice de confusion (heatmap) + `classification_report` dÃ©taillÃ©
- Affichage du seuil courant au bas de la page.

Lâ€™app illustre lâ€™usage pratique du pipeline et du seuil optimisÃ© sur validation.

---

## ğŸ§© Logique de boutâ€‘enâ€‘bout

1) PrÃ©paration des donnÃ©es
- Lecture CSV, nettoyage, `label_bin`.
- Splits stratifiÃ©s reproductibles: train/val/test.

2) ReprÃ©sentation & ModÃ©lisation
- Texte â†’ `TfidfVectorizer` (min_df, nâ€‘grammes) â†’ modÃ¨le (LR/LinearSVC calibrÃ©/NB/SVM RBF).
- Scoring par Average Precision (AP) en CV pour coller au dÃ©sÃ©quilibre et Ã  la qualitÃ© PR.

3) SÃ©lection de seuil
- Sur la validation, on choisit le seuil qui maximise la F1 via la courbe Precisionâ€‘Recall.
- Ce seuil est rÃ©utilisÃ© Ã  lâ€™infÃ©rence (Streamlit + scripts), Ã©vitant lâ€™hypothÃ¨se 0.5.

4) Ã‰valuation & Persistance
- MÃ©triques complÃ¨tes sur test, ROC/PR, matrice de confusion.
- Sauvegarde `model.pkl` + `threshold.txt`.

5) DÃ©ploiement local
- App Streamlit charge le pipeline et le seuil, propose prÃ©diction unitaire et par CSV, avec visualisations.

---

## ğŸ› ï¸ Installation & ExÃ©cution

1) Environnement
```bash
python -m venv .venv
source .venv/bin/activate    # Windows: .venv\Scripts\activate
pip install -r requirements.txt
```

2) EntraÃ®nement
```bash
python -m src.train
```
RÃ©sultats: `models/model.pkl` et `models/threshold.txt` + mÃ©triques test.

3) Ã‰valuation rapide sur tout le CSV
```bash
python -m src.evaluate
```

4) Lancer lâ€™application Streamlit
```bash
streamlit run app/streamlit_app.py
```

---

## âœ… Tests

- `tests/test_model.py`: tests simples de chargement et dâ€™existence des outputs. Ã‰tendre pour:
  - valider lâ€™API dâ€™infÃ©rence (probas/decision)
  - vÃ©rifier la prÃ©sence de `threshold.txt` et la compatibilitÃ© avec le modÃ¨le
  - contrÃ´ler que lâ€™entraÃ®nement produit des mÃ©triques minimales (e.g., F1 > 0.9 sur ce dataset)

---

## ğŸ” Choix techniques et justifications

- **AP (Average Precision) en CV**: mesure adaptÃ©e au dÃ©sÃ©quilibre; optimise la zone sous PR.
- **class_weight / calibration / seuil**: trio pour mieux capturer les spams avec coÃ»t asymÃ©trique.
- **Seuil appris â‰  0.5**: en pratique, 0.5 nâ€™est pas optimal; on choisit le seuil max F1 sur validation et on lâ€™applique partout.
- **Fallback probas**: `decision_function` + minâ€‘max pour les modÃ¨les non probabilistes.
- **Pipeline scikitâ€‘learn**: reproductible, sÃ©rialisable (`joblib`), compatible GridSearch.

---

## ğŸš§ AmÃ©liorations possibles

- Enrichir la prÃ©â€‘proc (lowercase, strip headers/signatures, suppression de tokens trÃ¨s frÃ©quents, normalisation dâ€™URLs et de nombres, stopwords multilingues, stemming/lemmatisation optionnels).
- Essayer des modÃ¨les linÃ©aires rÃ©gularisÃ©s (SGDClassifier log/hinge), ou des encoders plus riches (HashingVectorizer, char nâ€‘grams Ã©tendus).
- Calibration isotonic/platt globale postâ€‘sÃ©lection.
- Courbes ROC/PR intÃ©grÃ©es dans lâ€™app (upload avec labels) + sÃ©lection de seuil interactive.
- IntÃ©gration DVC endâ€‘toâ€‘end (pipelines) et CI (tests + lint + mÃ©triques minimales).

---

## ğŸ§¾ Licence & Auteurs

Usage pÃ©dagogique. Contributions bienvenues pour tester dâ€™autres modÃ¨les, enrichir lâ€™app, ou amÃ©liorer la robustesse production.
